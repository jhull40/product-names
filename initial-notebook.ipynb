{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "\n",
    "## Process\n",
    "1. read data and check language, lengths, BoW\n",
    "2. process/standardize\n",
    "3. mannheem data?\n",
    "\n",
    "## Human in loop \n",
    "1. topic model\n",
    "2. color / size lookups\n",
    "\n",
    "## Emebbing / Prediction Models\n",
    "1. stacked targets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "a.) If we want to understand which catalog (such as clothing, shoes, accessories, beauty, jewelry etc.) each item is, how will you make that happen?\n",
    "\n",
    "\n",
    "b.) How can you extract the additional information from the item names, such as the color, style, size, material, gender etc. if there is any?\n",
    "\n",
    "\n",
    "c.) A plus. If you write the queries/codes, or build a machine learning model to achieve the goal of a.) or b.) above on the attached dataset. Python is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from colour import Color\n",
    "from gensim import models, corpora\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ecommerce_product_names.csv', header=0, names=['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AW Bellies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sicons All Purpose Arnica Dog Shampoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eternal Gandhi Super Series Crystal Paper Weig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw\n",
       "0                Alisha Solid Women's Cycling Shorts\n",
       "1                FabHomeDecor Fabric Double Sofa Bed\n",
       "2                                         AW Bellies\n",
       "3              Sicons All Purpose Arnica Dog Shampoo\n",
       "4  Eternal Gandhi Super Series Crystal Paper Weig..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data functions\n",
    "\n",
    "GENDERS = ['boy', 'girl', 'kid', 'baby', 'infant', 'child', 'dog', 'cat', 'man', 'woman', 'pet']\n",
    "\n",
    "\n",
    "def clean_raw_text(raw_text):\n",
    "    \"\"\"\n",
    "    Clean Raw Text\n",
    "    \n",
    "    Removes punctuation and lowercases all letters\n",
    "    \n",
    "    \n",
    "    args:\n",
    "        raw_text: string of one/multiple tokens to be cleaned\n",
    "        \n",
    "    returns:\n",
    "        clean_text: cleaned version of raw_text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    clean_text = raw_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def get_lemmas(raw_text):\n",
    "    \"\"\"\n",
    "    Get Lemmas\n",
    "    \n",
    "    Uses nltk's lemmatizer to find lemmas for given string. \n",
    "    Because the lemmatizer only takes one word at a time, the function\n",
    "    splits the string, then joins it again\n",
    "    \n",
    "    args:\n",
    "        raw_text: string of one/multiple tokens to be lemmatized\n",
    "        \n",
    "    returns:\n",
    "        lemmas: lemmatized version of raw_text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = ' '.join([lemmatizer.lemmatize(word) for word in raw_text.split()])\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def check_if_color(token):\n",
    "    \"\"\"\n",
    "    Check if Color\n",
    "    \n",
    "    Helper function to detect color for a given word\n",
    "    \n",
    "    args:\n",
    "        token: an unknown word (string)\n",
    "        \n",
    "    returns:\n",
    "        True if token is a color, else False\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        Color(token)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def get_colors(raw_text):\n",
    "    \"\"\"\n",
    "    Get Colors\n",
    "    \n",
    "    Calls check_if_color for every token in string passed to it\n",
    "    Removes duplicates (white white) from same description\n",
    "    \n",
    "    args:\n",
    "        raw_text: string to check for colors\n",
    "        \n",
    "    returns:\n",
    "        colors: string of colors found in raw_text\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    colors = list(set([word for word in raw_text.split() if check_if_color(word)]))\n",
    "    \n",
    "    return colors\n",
    "\n",
    "\n",
    "def get_gender(raw_text, genders_list):\n",
    "    \"\"\"\n",
    "    Get Gender\n",
    "    \n",
    "    Uses pre-defined lookup table to find gender. \n",
    "    Removes duplicates (woman woman) from same description\n",
    "    \n",
    "    args:\n",
    "        raw_text: string to check for gender\n",
    "        genders_list: list of genders to search for\n",
    "        \n",
    "    returns:\n",
    "        genders: list of genders found in raw_text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    genders = list(set([word for word in raw_text.split() if word in genders_list]))\n",
    "    \n",
    "    return genders\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean'] = data['raw'].apply(lambda x: clean_raw_text(x))\n",
    "data['lemmas'] = data['clean'].apply(lambda x: get_lemmas(x))\n",
    "data['color'] = data['lemmas'].apply(lambda x: get_colors(x))\n",
    "data['gender'] = data['lemmas'].apply(lambda x: get_gender(x, GENDERS))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>clean</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>color</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>alisha solid womens cycling shorts</td>\n",
       "      <td>alisha solid woman cycling short</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "      <td>fabhomedecor fabric double sofa bed</td>\n",
       "      <td>fabhomedecor fabric double sofa bed</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AW Bellies</td>\n",
       "      <td>aw bellies</td>\n",
       "      <td>aw belly</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sicons All Purpose Arnica Dog Shampoo</td>\n",
       "      <td>sicons all purpose arnica dog shampoo</td>\n",
       "      <td>sicons all purpose arnica dog shampoo</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dog]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eternal Gandhi Super Series Crystal Paper Weig...</td>\n",
       "      <td>eternal gandhi super series crystal paper weig...</td>\n",
       "      <td>eternal gandhi super series crystal paper weig...</td>\n",
       "      <td>[silver]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12618</th>\n",
       "      <td>Purple Women Heels</td>\n",
       "      <td>purple women heels</td>\n",
       "      <td>purple woman heel</td>\n",
       "      <td>[purple]</td>\n",
       "      <td>[woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>Uberlyfe Large Vinyl Sticker</td>\n",
       "      <td>uberlyfe large vinyl sticker</td>\n",
       "      <td>uberlyfe large vinyl sticker</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12620</th>\n",
       "      <td>We Witches Comfy Hues Women Wedges</td>\n",
       "      <td>we witches comfy hues women wedges</td>\n",
       "      <td>we witch comfy hue woman wedge</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12621</th>\n",
       "      <td>Stylistry Women Heels</td>\n",
       "      <td>stylistry women heels</td>\n",
       "      <td>stylistry woman heel</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12622</th>\n",
       "      <td>Uberlyfe Extra Large Vinyl Sticker</td>\n",
       "      <td>uberlyfe extra large vinyl sticker</td>\n",
       "      <td>uberlyfe extra large vinyl sticker</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12623 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     raw  \\\n",
       "0                    Alisha Solid Women's Cycling Shorts   \n",
       "1                    FabHomeDecor Fabric Double Sofa Bed   \n",
       "2                                             AW Bellies   \n",
       "3                  Sicons All Purpose Arnica Dog Shampoo   \n",
       "4      Eternal Gandhi Super Series Crystal Paper Weig...   \n",
       "...                                                  ...   \n",
       "12618                                 Purple Women Heels   \n",
       "12619                       Uberlyfe Large Vinyl Sticker   \n",
       "12620                 We Witches Comfy Hues Women Wedges   \n",
       "12621                              Stylistry Women Heels   \n",
       "12622                 Uberlyfe Extra Large Vinyl Sticker   \n",
       "\n",
       "                                                   clean  \\\n",
       "0                     alisha solid womens cycling shorts   \n",
       "1                    fabhomedecor fabric double sofa bed   \n",
       "2                                             aw bellies   \n",
       "3                  sicons all purpose arnica dog shampoo   \n",
       "4      eternal gandhi super series crystal paper weig...   \n",
       "...                                                  ...   \n",
       "12618                                 purple women heels   \n",
       "12619                       uberlyfe large vinyl sticker   \n",
       "12620                 we witches comfy hues women wedges   \n",
       "12621                              stylistry women heels   \n",
       "12622                 uberlyfe extra large vinyl sticker   \n",
       "\n",
       "                                                  lemmas     color   gender  \n",
       "0                       alisha solid woman cycling short        []  [woman]  \n",
       "1                    fabhomedecor fabric double sofa bed        []       []  \n",
       "2                                               aw belly        []       []  \n",
       "3                  sicons all purpose arnica dog shampoo        []    [dog]  \n",
       "4      eternal gandhi super series crystal paper weig...  [silver]       []  \n",
       "...                                                  ...       ...      ...  \n",
       "12618                                  purple woman heel  [purple]  [woman]  \n",
       "12619                       uberlyfe large vinyl sticker        []       []  \n",
       "12620                     we witch comfy hue woman wedge        []  [woman]  \n",
       "12621                               stylistry woman heel        []  [woman]  \n",
       "12622                 uberlyfe extra large vinyl sticker        []       []  \n",
       "\n",
       "[12623 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data[data['color'].str.contains('blue')]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO one hot encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prep_data_for_tm(data):\n",
    "    docs = []\n",
    "    \n",
    "    for i in range(len(data.index)):\n",
    "        docs.append(data.loc[i,'tm_text'].split())\n",
    "\n",
    "\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    return corpus, dictionary, docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tm_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tm_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a1f9afb0ec5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tm_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tm_text'"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        \n",
    " \n",
    "data1 = data['tm_text'].values.tolist()\n",
    "data_words = list(sent_to_words(data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>clean</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>color</th>\n",
       "      <th>gender</th>\n",
       "      <th>tm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
       "      <td>alisha solid womens cycling shorts</td>\n",
       "      <td>alisha solid woman cycling short</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>alisha solid woman cycling short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
       "      <td>fabhomedecor fabric double sofa bed</td>\n",
       "      <td>fabhomedecor fabric double sofa bed</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>fabhomedecor fabric double sofa bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AW Bellies</td>\n",
       "      <td>aw bellies</td>\n",
       "      <td>aw belly</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>aw belly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sicons All Purpose Arnica Dog Shampoo</td>\n",
       "      <td>sicons all purpose arnica dog shampoo</td>\n",
       "      <td>sicons all purpose arnica dog shampoo</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>sicons purpose arnica dog shampoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eternal Gandhi Super Series Crystal Paper Weig...</td>\n",
       "      <td>eternal gandhi super series crystal paper weig...</td>\n",
       "      <td>eternal gandhi super series crystal paper weig...</td>\n",
       "      <td>[silver]</td>\n",
       "      <td>[]</td>\n",
       "      <td>eternal gandhi super series crystal paper weig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12618</th>\n",
       "      <td>Purple Women Heels</td>\n",
       "      <td>purple women heels</td>\n",
       "      <td>purple woman heel</td>\n",
       "      <td>[purple]</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>purple woman heel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>Uberlyfe Large Vinyl Sticker</td>\n",
       "      <td>uberlyfe large vinyl sticker</td>\n",
       "      <td>uberlyfe large vinyl sticker</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>uberlyfe large vinyl sticker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12620</th>\n",
       "      <td>We Witches Comfy Hues Women Wedges</td>\n",
       "      <td>we witches comfy hues women wedges</td>\n",
       "      <td>we witch comfy hue woman wedge</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>witch comfy hue woman wedge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12621</th>\n",
       "      <td>Stylistry Women Heels</td>\n",
       "      <td>stylistry women heels</td>\n",
       "      <td>stylistry woman heel</td>\n",
       "      <td>[]</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>stylistry woman heel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12622</th>\n",
       "      <td>Uberlyfe Extra Large Vinyl Sticker</td>\n",
       "      <td>uberlyfe extra large vinyl sticker</td>\n",
       "      <td>uberlyfe extra large vinyl sticker</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>uberlyfe extra large vinyl sticker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12623 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     raw  \\\n",
       "0                    Alisha Solid Women's Cycling Shorts   \n",
       "1                    FabHomeDecor Fabric Double Sofa Bed   \n",
       "2                                             AW Bellies   \n",
       "3                  Sicons All Purpose Arnica Dog Shampoo   \n",
       "4      Eternal Gandhi Super Series Crystal Paper Weig...   \n",
       "...                                                  ...   \n",
       "12618                                 Purple Women Heels   \n",
       "12619                       Uberlyfe Large Vinyl Sticker   \n",
       "12620                 We Witches Comfy Hues Women Wedges   \n",
       "12621                              Stylistry Women Heels   \n",
       "12622                 Uberlyfe Extra Large Vinyl Sticker   \n",
       "\n",
       "                                                   clean  \\\n",
       "0                     alisha solid womens cycling shorts   \n",
       "1                    fabhomedecor fabric double sofa bed   \n",
       "2                                             aw bellies   \n",
       "3                  sicons all purpose arnica dog shampoo   \n",
       "4      eternal gandhi super series crystal paper weig...   \n",
       "...                                                  ...   \n",
       "12618                                 purple women heels   \n",
       "12619                       uberlyfe large vinyl sticker   \n",
       "12620                 we witches comfy hues women wedges   \n",
       "12621                              stylistry women heels   \n",
       "12622                 uberlyfe extra large vinyl sticker   \n",
       "\n",
       "                                                  lemmas     color   gender  \\\n",
       "0                       alisha solid woman cycling short        []  [woman]   \n",
       "1                    fabhomedecor fabric double sofa bed        []       []   \n",
       "2                                               aw belly        []       []   \n",
       "3                  sicons all purpose arnica dog shampoo        []    [dog]   \n",
       "4      eternal gandhi super series crystal paper weig...  [silver]       []   \n",
       "...                                                  ...       ...      ...   \n",
       "12618                                  purple woman heel  [purple]  [woman]   \n",
       "12619                       uberlyfe large vinyl sticker        []       []   \n",
       "12620                     we witch comfy hue woman wedge        []  [woman]   \n",
       "12621                               stylistry woman heel        []  [woman]   \n",
       "12622                 uberlyfe extra large vinyl sticker        []       []   \n",
       "\n",
       "                                                 tm_text  \n",
       "0                       alisha solid woman cycling short  \n",
       "1                    fabhomedecor fabric double sofa bed  \n",
       "2                                               aw belly  \n",
       "3                      sicons purpose arnica dog shampoo  \n",
       "4      eternal gandhi super series crystal paper weig...  \n",
       "...                                                  ...  \n",
       "12618                                  purple woman heel  \n",
       "12619                       uberlyfe large vinyl sticker  \n",
       "12620                        witch comfy hue woman wedge  \n",
       "12621                               stylistry woman heel  \n",
       "12622                 uberlyfe extra large vinyl sticker  \n",
       "\n",
       "[12623 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tm_text'] = data['lemmas'].apply(lambda x: remove_stopwords(x, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f3a1b14aea1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# higher threshold fewer phrases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "STOP_WORDS.extend(['from', 'set', 'use'])\n",
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(raw_text, stop_words):\n",
    "    \n",
    "    tm_text = ' '.join([word for word in raw_text.split() if word not in stop_words])\n",
    "    \n",
    "    return tm_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    \n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words, STOP_WORDS)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "data_words_trigrams = make_trigrams(make_bigrams(data_words))\n",
    "#data_words_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(data_words_trigrams)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in data_words_trigrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus, \n",
    " num_topics=20, \n",
    " id2word=dictionary, \n",
    " distributed=False, \n",
    " chunksize=200, \n",
    " passes=5, \n",
    " update_every=1, \n",
    " alpha='auto', \n",
    " eta=None, \n",
    " decay=0.5, \n",
    " offset=1.0, \n",
    " eval_every=10, \n",
    " iterations=50, \n",
    " gamma_threshold=0.001, \n",
    " minimum_probability=0.01, \n",
    " random_state=2, \n",
    " ns_conf=None, \n",
    " minimum_phi_value=0.01, \n",
    " per_word_topics=False, \n",
    " callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_visualization = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, sort_topics=False, n_jobs=1, mds='mmds')\n",
    "pyLDAvis.display(lda_visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for i in data.index:\n",
    "    words = words + data.loc[i,'tm_text'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list):\n",
    "     \n",
    "   # Creating an empty dictionary\n",
    "   count = {}\n",
    "   for i in my_list:\n",
    "    count[i] = count.get(i, 0) + 1\n",
    "   return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfrq = CountFrequency(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.loc[2, 'tm_text']\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, \n",
    "written in the programming languages Python and Cython. The library is published under the MIT license\n",
    "and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1, 'tm_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['kw_spacy'] = data['tm_text'].apply(lambda x: nlp(x).ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yake\n",
    "import yake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alisha solid', 0.04940384002065631)\n",
      "('cycling short', 0.04940384002065631)\n",
      "('solid woman', 0.09700399286574239)\n",
      "('woman cycling', 0.09700399286574239)\n",
      "('alisha', 0.15831692877998726)\n",
      "('short', 0.15831692877998726)\n",
      "('solid', 0.29736558256021506)\n",
      "('woman', 0.29736558256021506)\n",
      "('cycling', 0.29736558256021506)\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "\n",
    "\n",
    "text = data.loc[0, 'tm_text']\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fabhomedecor fabric double sofa bed']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "text = \"\"\"spaCy is an open-source software library for advanced natural language processing,\n",
    "written in the programming languages Python and Cython. The library is published under the MIT license\n",
    "and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\"\"\"\n",
    "\n",
    "text = data.loc[1, 'tm_text']\n",
    "rake_nltk_var.extract_keywords_from_text(text)\n",
    "keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "print(keyword_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
